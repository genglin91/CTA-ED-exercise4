---
title: "CTA-ED Exercise 4: Scaling techniques (with correct answers)"
author: "Lin Geng"
date: "27/02/2025"
output: html_document
---

# Introduction

The hands-on exercise for this week focuses on: 1) scaling texts ; 2) implementing scaling techniques using `quanteda`. 

In this tutorial, you will learn how to:
  
* Scale texts using the "wordfish" algorithm
* Scale texts gathered from online sources
* Replicate analyses by @kaneko_estimating_2021

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
library(quanteda.textplots) #for visualizing text modelling results
```

In this exercise we'll be using the dataset we used for the sentiment analysis exercise. The data were collected from the Twitter accounts of the top eight newspapers in the UK by circulation. The tweets include any tweets by the news outlet from their main account. 

## Importing data

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
```

We first take a sample from these data to speed up the runtime of some of the analyses. 

```{r}
tweets <- tweets %>%
  sample_n(20000)
```

## Construct `dfm` object

Then, as in the previous exercise, we create a corpus object, specify the document-level variables by which we want to group, and generate our document feature matrix. 

```{r}
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "text")

#add in username document-level information
docvars(tweets_corpus, "newspaper") <- tweets$user_username

dfm_tweets <- dfm(tokens(tweets_corpus,
                    remove_punct = TRUE)) %>%
  dfm_select(pattern = stopwords("english"), 
             selection = "remove",
             valuetype = "fixed")
```

We can then have a look at the number of documents (tweets) we have per newspaper Twitter account. 

```{r}

## number of tweets per newspaper
table(docvars(dfm_tweets, "newspaper"))

```

And this is what our document feature matrix looks like, where each word has a count for each of our eight newspapers. 

```{r}

dfm_tweets

```

## Estimate wordfish model

Once we have our data in this format, we are able to group and trim the document feature matrix before estimating the wordfish model.

```{r}
# compress the document-feature matrix at the newspaper level
dfm_newstweets <- dfm_group(dfm_tweets, groups = newspaper)

# remove words not used by two or more newspapers
dfm_newstweets <- dfm_trim(dfm_newstweets, 
                                min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(dfm_newstweets)

#### estimate the Wordfish model ####
set.seed(123L)
dfm_newstweets_results <- textmodel_wordfish(dfm_newstweets)
                                          
# unused argument (sparse = TRUE)
```

And this is what results.

```{r}
summary(dfm_newstweets_results)
```

We can then plot our estimates of the $\theta$s---i.e., the estimates of the latent newspaper position---as so.

```{r}
textplot_scale1d(dfm_newstweets_results)
```

Interestingly, we seem not to have captured ideology but some other tonal dimension. We see that the tabloid newspapers are scored similarly, and grouped toward the right hand side of this latent dimension; whereas the broadsheet newspapers have an estimated theta further to the left.

Plotting the "features," i.e., the word-level betas shows how words are positioned along this dimension, and which words help discriminate between news outlets.

```{r}

textplot_scale1d(dfm_newstweets_results, margin = "features")

```

And we can also look at these features.

```{r}

features <- dfm_newstweets_results[["features"]]

betas <- dfm_newstweets_results[["beta"]]

feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")

```

These words do seem to belong to more tabloid-style reportage, and include emojis relating to film, sports reporting on "cristiano" as well as more colloquial terms like "saucy."

## Replicating Kaneko et al.

This section adapts code from the replication data provided for @kaneko_estimating_2021 [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EL3KYD). 


If you're working locally, you can download the `dfm` data with:

```{r}
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))
```

This data is in the form a document-feature-matrix. We can first manipulate it in the same way as @kaneko_estimating_2021 by grouping at the level of newspaper and removing infrequent words.

```{r}
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(kaneko_dfm_study1)
```

## Exercises

1. Estimate a wordfish model for with Kaneko (2021)'s data

```{r}
set.seed(123L)
kaneko_dfm_study1_results <- textmodel_wordfish(kaneko_dfm_study1)

summary(kaneko_dfm_study1_results)

```

The beta value represents the extent to which a word differentiates positions on the latent ideological scale, while the psi value reflects the overall frequency of the word.

A positive beta tends to indicate support for conservative or government-aligned positions, such as "安倍首相"(Prime Minister Abe) (0.2524), "安全保障" (security)(0.2303), and "集団的自衛権"(collective self-Defense) (0.2183). In contrast, a negative beta is associated with criticism of the government or a liberal stance, as seen in "大改革"(great reform) (-1.362), "成就"(achievement) (-2.335), and "歴史的"(historical) (-1.154).

Words with high psi values, such as "安保"(security) (3.6911), "集団的自衛権"(collective self-defense) (3.7932), and "国会"(congress) (3.7344), indicate that they are widely used across media outlets regardless of ideological stance when discussing security policy. On the other hand, words with low psi values, such as "歴史的" (historical)(-2.105) and "成就" (achievement) (-2.481), may carry specific ideological connotations in biased reporting. 

Looking at the theta values, Yomiuri (1.604), Sankei (1.247), and Nikkei (1.258) are positioned on the positive end of the ideological scale, suggesting that these newspapers are more likely to support government positions or conservative perspectives on security policy. Conversely, Chunichi (-1.223), Asahi (-0.989), and Chugoku (-0.459) are on the negative end, indicating a tendency to criticize government policies or adopt a more liberal stance on security issues.

Notably, Mainichi (-0.188) and Nishinippon (-0.262) have theta values closer to zero, suggesting that they are relatively neutral or less ideologically distinct on this dimension.

Additionally, the standard error (se) reflects the uncertainty of these estimates. Newspapers like Nikkei and Sankei have larger standard errors, which may indicate greater variability in their positions. In contrast, liberal newspapers like Chunichi and Asahi have smaller standard errors, suggesting a more consistent stance on this issue.


2. Visualize the results
```{r, fig.cap='Wordfish model estimates of Japanese newspapers editorial texts' }
## We can then plot our estimates of the thetas---i.e., the estimates of the latent Japanese newspaper position.
textplot_scale1d(kaneko_dfm_study1_results)

#textplot_scale1d(kaneko_dfm_study1_results, margin = "features")
#cannot show the Japanese characters
```

The plot displays the estimated ideological positions of different newspapers along a latent dimension. 

Yomiuri, Sankei, and Nikkei are located on the positive end, indicating a tendency to support government policies, particularly in the context of security issues.
Asahi, Chunichi, and Chugoku are on the negative end, suggesting a more critical or liberal position on these topics.
Mainichi and Nishinippon, positioned closer to the center, appear to be more neutral or moderate in their editorial stance.


```{r, fig.cap='Wordfish model estimates of Japanese newspapers editorial texts'}
kaneko_features <- kaneko_dfm_study1_results[["features"]]

kaneko_betas <- kaneko_dfm_study1_results[["beta"]]

kaneko_feat_betas <- as.data.frame(cbind(kaneko_features, kaneko_betas))
kaneko_feat_betas$kaneko_betas <- as.numeric(kaneko_feat_betas$kaneko_betas)

kaneko_feat_betas %>%
  arrange(desc(kaneko_betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")
```
These words are more capable of distinguishing different ideological positions.  

"国際平和協力" (International Peace Cooperation), "世界平和" (world peace), "世界秩序" (world order): These words relate to international peace and global order, commonly appearing in reports on foreign policy and national security. "環太平洋経済連携協定" (Trans-Pacific Partnership) (TPP): This is an economic policy-related term, likely associated with the government's promotion of free trade policies. "消費税率" (consumption tax rate), "基礎的財政収支" (primary balance), "黒字" (summary): These words concern fiscal policy and taxation issues, which may be used differently by media outlets with different ideological positions in economic reporting. 

Their high beta values indicate that their usage may vary across media with different ideological stances, particularly in economic policy debates.



